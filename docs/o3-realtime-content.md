Great, I’ll explore how the `flotob/curia` repo currently handles data flow and API interactions, and design a real-time architecture that uses Socket.IO and Redis, hosted on Railway. I’ll prioritize a self-hosted setup that integrates well with your existing serverless Next.js + PostgreSQL stack, and respects your JWT-based permission model.

I’ll also recommend implementation patterns for real-time features like board-based rooms, typing indicators, and voting updates, along with Redis integration and syncing permissions securely.

I’ll update you as soon as the architecture plan and recommendations are ready.


# Designing a Real-Time System for Curia (MVP Architecture)

## Architecture Overview and Data Flow

**Current Setup:** Curia’s existing stack uses Next.js API routes (serverless functions) for data operations, a PostgreSQL database for persistence, and TanStack Query on the client for caching and mutations. Authentication is via JWT tokens generated by the Next.js backend (including user `sub` (ID), `cid` (community ID), role IDs, and an admin flag). All client API calls include this JWT for authorization via a custom `withAuth` middleware on the server. The API endpoints validate the JWT, attach user info (userId, roles, community, isAdmin) to the request, and enforce permissions (e.g. ensuring users only access their community’s data and only boards they have roles for). After database operations, the client typically invalidates or updates TanStack Query caches to reflect changes (e.g. refreshing the post list after a new post or updating vote counts).

**Proposed Real-Time Extension:** We introduce a Socket.IO server running alongside the Next.js app, plus a Redis instance for pub/sub. The Next.js server (running in a Railway container) will host both the API routes *and* the Socket.IO service. Each Next.js instance will attach a Socket.IO server to its HTTP server (using a custom server or an API route hack) and use the **Redis adapter** to join a pub/sub network. This ensures real-time events (new posts, comments, votes, etc.) broadcast on one instance reach clients connected to any instance. The diagram below illustrates the high-level architecture with multiple app instances and Redis as a message broker:

&#x20;*Multiple Next.js server instances using Socket.IO with a Redis adapter to synchronize events.*

In this setup, when a user performs an action (e.g. posts a comment) via the REST API, the server both writes to the database **and** emits a Socket.IO event to the relevant room. The Redis adapter ensures all listening clients (even on other instances) receive the event. Conversely, if an event is triggered on one server (or even by an external process), it’s forwarded through Redis to all Socket.IO servers, keeping connected clients in sync.

## Socket.IO Integration with Next.js and Redis

To integrate Socket.IO into the Next.js app (which is containerized on Railway), we can initialize the Socket.IO server once when the app starts. One approach is to create a custom server (e.g. using Express) that handles Next.js requests and also attaches a Socket.IO server. Another simpler MVP approach is to use a Next.js API route for initialization – for example, an API route that checks if `res.socket.server.io` exists, and if not, creates a new `Server(httpServer)` and stores it. This way, the Socket.IO server piggybacks on Next’s underlying Node HTTP server. We then apply the Redis adapter:

```js
import { createClient } from "redis";
import { createAdapter } from "@socket.io/redis-adapter";
import { Server } from "socket.io";

// Pseudocode: run this in server startup or a singleton API route
const io = new Server(httpServer, { /* CORS options if needed */ });
const pubClient = createClient({ url: process.env.REDIS_URL });
const subClient = pubClient.duplicate();
await Promise.all([pubClient.connect(), subClient.connect()]);
io.adapter(createAdapter(pubClient, subClient));
```

This attaches Socket.IO to the app and uses Redis for scaling. **Rooms** in Socket.IO will represent boards and possibly other contexts. We’ll have the server join clients to rooms based on board IDs (more on that below).

All Socket.IO servers will subscribe to a common Redis Pub/Sub channel. When one server calls `io.to(<room>).emit('event', data)`, the adapter publishes the message via Redis; other server instances then relay it to their clients in that room. The Redis adapter diagram above shows how messages propagate without any state being permanently stored in Redis (it just routes events). This enables horizontal scaling: we can run multiple app instances on Railway, and real-time events still reach all subscribed users.

**Event Workflow:** For example, a **“newPost”** event flow would be: a user submits a post via REST → Next API handler creates the post in DB and returns it (to the caller) → that handler also emits `io.to(boardId).emit('newPost', newPostData)` to notify everyone in that board’s room. All clients listening in that board get the event in real-time. Similarly, for an upvote: the API route updates the vote in DB (increasing count) and emits `io.to(boardId).emit('postUpvoted', { postId, newCount, userId })`. Comments can emit `io.to(boardId).emit('newComment', { postId, comment: newComment })`, etc. Each event includes enough info for clients to update their UI (or at least an indication to refetch if needed).

## Authentication and Permission Enforcement for Socket Connections

**JWT Handshake:** We will reuse the existing JWT system for authenticating socket connections. On the client, when initiating the Socket.IO connection, include the JWT (e.g. as a query param or auth header in the handshake). On the server, use a Socket.IO middleware to verify this token before allowing connection:

```js
io.use((socket, next) => {
  const token = socket.handshake.auth?.token || socket.handshake.headers?.authorization;
  if (!token) return next(new Error("Unauthorized"));
  try {
    const decoded = jwt.verify(token, JWT_SECRET) as JwtPayload;
    socket.data.user = decoded; // attach user info (id, roles, cid, adm, etc.)
    return next();
  } catch (err) {
    return next(new Error("Unauthorized"));
  }
});
```

This mirrors the `withAuth` logic from the API (checking the `Authorization: Bearer ...` token and decoding it). If the JWT is invalid or missing, the socket connection is rejected with an error. On success, `socket.data.user` will hold the same info that `req.user` does in API routes (userId, communityId, roles array, isAdmin flag, etc.).

**Room Subscription & Authorization:** Once connected, the client can join specific board rooms. We will expose events like `socket.emit('joinBoard', boardId)` from client to server to request joining a board channel. The server handler for `joinBoard` will enforce that the user has access to that board’s content before joining them to the room. We can leverage the same permission checks used in REST endpoints. For example, on `joinBoard`:

```js
socket.on('joinBoard', async (boardId) => {
  const { cid, roles, adm } = socket.data.user;
  // 1. Verify the board belongs to the user's community (cid)
  const board = await db.query('SELECT settings, community_id FROM boards WHERE id=$1', [boardId]);
  if (!board || board.community_id !== cid) {
    return socket.emit('error', 'Board not found or forbidden');
  }
  // 2. Check access via roles and settings
  const settings = typeof board.settings === 'string' ? JSON.parse(board.settings) : board.settings;
  const canAccess = canUserAccessBoard(roles, settings, !!adm);
  if (!canAccess) {
    return socket.emit('error', 'Access denied');
  }
  // 3. Join room if authorized
  socket.join(`board_${boardId}`);
  // (Optionally emit an event confirming subscription or current presence info)
});
```

This uses the same logic as server APIs when filtering boards or verifying access. If the user’s JWT roles and admin status don’t allow access, the join is refused (we notify the client or simply not send them data for that board). For convenience, the client might automatically join all boards it has access to (for a “home feed” across boards). However, a more efficient approach is to join rooms on demand (e.g. when user enters a board’s page or opens a post). An MVP compromise: on connection, the server could auto-join the user to all accessible boards in their community to guarantee they receive all relevant events. The list of accessible boards can be retrieved similarly to the REST call (e.g., using the existing `filterAccessibleBoards` logic). This might be fine if the number of boards is moderate. Otherwise, on navigation the client can explicitly join/leave board rooms.

**Emitting to Rooms:** After this setup, server-side event broadcasting will use these rooms. For example, a “newPost” in board 123 will be emitted with `io.to('board_123').emit('newPost', postData)`. Only users who joined `board_123` (and by construction, only authorized users can join it) will receive it. This ensures *role-based data isolation* – events for a private board never go to users without permission. The JWT’s `cid` (community ID) also ensures a user can’t subscribe to a board of another community: the check above forbids it (unless the user is an admin overriding community scoping).

## Live Updates: New Posts, Comments, and Upvotes

With the above infra, **live updates** are straightforward:

* **New Posts:** When a post is created via the API (`POST /api/posts`), after writing to the DB the server emits a `newPost` event to the board’s room. The event payload can be the new post object (id, title, etc.) or a minimal info (id and maybe aggregated fields). Since the API already returns the full `ApiPost` to the creator, we can reuse that object for broadcasting. All clients listening on that board get the event in real-time. Clients can then update their UI: for instance, in the feed, if the user is viewing the board of the new post, the app can insert the new post into the list immediately. If the user is on the “all boards” feed (subscribed to all), they’ll get it as well. We might choose to *not* send the full content if not needed (to reduce payload), but as an MVP simplicity, sending the same JSON as the REST response is fine.

* **New Comments:** Similar to posts, when a comment is added (`POST /api/posts/:id/comments`), the server emits `newComment` to the post’s board room (e.g. `board_123`). We include at least the `postId` and the comment data. Clients can check if they are currently viewing that post’s comments – if yes, append the new comment to the list; if not (e.g. just seeing a post preview in a list), they might increment a comment counter or mark the post as having new activity. In our case, the feed shows `comment_count`, so the event could also carry an updated count. Alternatively, clients can simply invalidate their comment query cache for that post to fetch fresh data when needed.

* **Upvotes:** When an upvote or undo happens via `POST/DELETE /api/posts/:id/votes`, broadcast a `postVoted` (or separate `upvoteAdded`/`upvoteRemoved`) event to the board room. The server can send down the postId and new upvote count, and possibly which user voted if needed. Given the app already returns the updated post with new `upvote_count` and `user_has_upvoted` to the actor, broadcasting that updated snippet to others lets every client update that post’s displayed score. If a user is viewing the feed, they can update the specific post’s upvote count in their list data. If they have that post open individually, update it there as well.

In all cases, **TanStack Query integration** on the client will be used to merge these real-time updates into the cached data (discussed more below). Notably, the user performing the action already gets an immediate UI response via optimistic updates and query invalidation (as implemented currently). The real-time events mainly benefit *other users* (and other browser tabs) so they don’t have to wait for a periodic refetch.

## Presence and Typing Indicators

**Presence:** With Socket.IO rooms, we can track which users are online and where. When a user connects and authenticates, we know their userId and community. We can maintain an in-memory or Redis-based presence map keyed by board or by community. A simple approach: use Socket.IO’s room membership as presence. For example, the number of sockets in a `board_X` room is the number of online users currently “subscribed” to that board’s updates. We can leverage Socket.IO’s adapter API to get counts or use a separate structure for more detail. For MVP, we can broadcast presence changes: when a user joins a board room, emit an event like `userJoinedBoard` to that room with the user’s name/ID; when they disconnect or leave, emit `userLeftBoard`. This allows a basic “X users online” indicator per board or showing a list of active users if desired. Each client can maintain a list of currently present users in the board (cleared or updated on events).

To implement this, add handlers in the Socket.IO server:

```js
io.on('connection', socket => {
  const user = socket.data.user;
  socket.on('joinBoard', boardId => {
    // ... (auth check as above)
    socket.join(`board_${boardId}`);
    io.to(`board_${boardId}`).emit('presence', { 
      event: 'join', 
      boardId, 
      user: { id: user.sub, name: user.name } 
    });
  });
  socket.on('leaveBoard', boardId => {
    socket.leave(`board_${boardId}`);
    io.to(`board_${boardId}`).emit('presence', { 
      event: 'leave', 
      boardId, 
      user: { id: user.sub } 
    });
  });
  socket.on('disconnect', () => {
    // On disconnect, for each board the socket was in, emit leave presence.
    for (const room of socket.rooms) {
      if (room.startsWith('board_')) {
        io.to(room).emit('presence', { event: 'leave', boardId: room.split('_')[1], user: { id: user.sub } });
      }
    }
  });
});
```

This is a rough outline: essentially notify others when a user joins/leaves. The client can listen for `presence` events to update counters or display a list of active users. For efficiency, we might throttle the frequency or combine multiple joins/leaves into one periodic update in the future, but real-time individual events are acceptable for an MVP.

**Typing Indicators:** Typing indicators show when a user is in the process of writing a post or comment. We can implement this with short-lived Socket.IO events. For example, if a user focuses the “New Post” input and starts typing, emit a `typing` event to the board room, e.g. `socket.emit('typing', { boardId, context: 'post', isTyping: true })`. Similarly, when they stop or post, send `isTyping: false`. For comments, the context might include the post ID: `{ boardId, postId, context: 'comment', isTyping: true }`. The server simply broadcasts these to the appropriate room (`board_X`). We don’t store typing status in any database – it’s ephemeral. Clients receiving a `typing` event can update UI for a brief period (e.g. show “User Y is typing…” and use a timer to hide it if no update after a few seconds).

To avoid spamming with every keystroke, the client should debounce these emissions (e.g. only send every 500ms or when state changes). The server can also throttle them: if a user’s last typing event was just a second ago, it might drop subsequent ones to the same room until a short interval passes. Since typing indicators are hinting UI, dropping some is fine if the user types very fast.

In summary, presence and typing use Socket.IO primarily: presence uses join/leave events and room membership, typing uses custom events. We will ensure that these also respect permissions (they naturally do, since they’re confined to the same board rooms).

## Client-Side Integration with TanStack Query

On the React client, we will integrate the real-time events with TanStack Query to keep the UI state consistent. The application already uses TanStack (React Query) for caching and optimistic updates. For example, after an upvote, the VoteButton mutation optimistically updates local state and then invalidates the `"posts"` query to refetch fresh data. We can improve this using socket events to eliminate some refetches and make updates truly live.

**Optimistic UI and Live Updates:** The combination works as follows: when a user triggers a mutation (like upvoting), the UI updates instantly (optimistically). The server confirms and broadcasts the event to others. The originator could skip handling the broadcast since they already updated (or use it as a double-check). Other clients receive the event and update their cache. For instance, if a `postVoted` event comes in with `{ postId, newCount }`, we use the QueryClient to update the cached posts list:

```js
socket.on('postVoted', ({ postId, newCount, userId }) => {
  queryClient.setQueriesData(['posts'], (oldData) => {
    if (!oldData) return oldData;
    // Assume infinite query pages structure
    return {
      ...oldData,
      pages: oldData.pages.map(page => ({
        ...page,
        posts: page.posts.map(p => 
          p.id === postId ? { ...p, upvote_count: newCount } : p
        )
      }))
    };
  });
});
```

This mirrors the logic already used in the VoteButton component’s `onSuccess` handler. In fact, the code in VoteButton does exactly this kind of cache update for the current user’s own vote action. Now we’ll apply similar updates when an event arrives from another user’s action. We would do analogous cache updates for new posts and comments: e.g., on `newPost` event, we can insert the new post into the appropriate query cache. If using infinite scroll, that might mean prepending to the first page or simply invalidating the query to fetch the latest page. A straightforward approach is to call `queryClient.invalidateQueries(['posts'])` when a new post event arrives, so the feed refetches (the cost is an extra API call, but given new post frequency is low, it’s acceptable for MVP). Similarly, on `newComment`, we can invalidate the comments query for that post (`['comments', postId]`) so that the next time the user views or refreshes comments, they get the latest. If we want instant insertion of the comment for those currently on the post, we can use `queryClient.setQueryData(['comments', postId], old => [...old, newComment])`. The exact strategy can be tuned per event type, balancing complexity and freshness.

**TanStack Query side effects on events:** The presence and typing events likely won't be stored in Query caches (they are transient UI states), so those can be handled by React state or context separately (e.g., a `usePresence(boardId)` hook that uses socket events to maintain a list of online users, etc.). The query system remains focused on actual data (posts, comments, votes).

In summary, the real-time system will work *with* TanStack Query rather than replacing it: socket events will trigger cache invalidations or updates, which in turn cause React components to re-render with new data. This way, we maintain the source-of-truth in the database and API, but minimize the staleness window by pushing updates instantly. The existing polling (`refetchInterval`) on some queries can potentially be turned off or made less frequent once sockets are reliably updating data.

## Reconnection and Session Persistence (Iframe Considerations)

Railway will keep the server running, and Socket.IO has built-in reconnection for clients. If a client temporarily loses connection or the page reloads, Socket.IO will attempt to reconnect automatically. We need to ensure that on reconnect, the client re-authenticates and re-joins the appropriate rooms.

**JWT Re-auth:** Since the JWT is stored in memory via AuthContext, on page reload it might be reacquired via the plugin’s session flow. But assuming the JWT is still valid (not expired) and stored (perhaps in a context or localStorage if implemented), the client should pass it again on reconnect. Socket.IO’s client can be configured to remember the token or we provide it each time (e.g., `io.connect({ auth: { token: currentJwt } })`). On the server, the same auth middleware will run for the new connection and attach user data.

**Room Rejoin:** Socket.IO does not automatically remember which rooms a client was in if they disconnect; that state is lost. We have a couple of strategies:

* The client application can track which boards (rooms) it was viewing and re-emit join requests after reconnect.
* Or we use Socket.IO’s **Connection State Recovery** feature which can be configured to restore rooms on reconnect. However, as of writing, the Redis adapter support for state recovery is not available (Socket.IO v4 introduced it but the Redis adapter note indicates no support for it yet in the version used). So, MVP approach is to handle it manually in the client logic.

A practical solution is to have the client, upon reconnect, emit an event like `rejoinAll` or simply repeat the same logic as initial connection (joining all accessible board rooms). We know the user’s accessible boards either from a cached list (the app already fetches the boards list on load) or can quickly refetch it. So on reconnect success, do something like:

```js
socket.on('connect', () => {
  accessibleBoards.forEach(b => socket.emit('joinBoard', b.id));
});
```

This ensures the user is back in the same subscriptions. The slight delay during reconnect might cause a momentary lapse in receiving events, but since events are transient, missing a few seconds is usually fine (and critical events would be picked up on next query refetch anyway). For an even smoother experience, we could implement a buffer on the server: e.g., when a user disconnects, keep their session data for a short time (a few seconds) in memory (keyed by user or `iframeUid`) and if they reconnect quickly, emit any missed events. However, that’s complex for MVP.

**Iframe and Session Resilience:** The mention of *iframe loads* suggests the app might be running inside an iframe in a parent application (Common Ground). The `iframeUid` in the JWT is likely a unique session identifier for the plugin instance. We can use this to avoid treating a rapid iframe reload as two separate users. For example, if a user’s iframe is reloaded (thus new socket connection with the same `uid` in JWT), we might choose to consider it the same session. This could matter for presence: without adjustment, a reload would emit a `userLeftBoard` then `userJoinedBoard` almost instantly. We could mitigate flicker by delaying the `leave` event slightly. For instance, on disconnect, instead of immediately broadcasting user left, mark the session as “pending disconnect” and give it e.g. 5 seconds to see if the same user `uid` reconnects. If they do, cancel the leave (since it was a quick reload). If not, then emit the leave. Implementing this would involve storing a map of `iframeUid -> lastSeen` timestamp or a reconnect timeout. Given `iframeUid` is included in the JWT and also stored in AuthContext, we can utilize it on the server side via `socket.data.user.uid`.

For MVP, it might be acceptable to ignore this nuance (the worst case is presence count blips for a moment). But it’s good to note this possibility. If a smoother experience is desired, a small server-side buffer for reconnects can be added using the above approach. This is somewhat analogous to “ghost ping” prevention in chat apps.

**Session Persistence:** As long as the JWT remains valid (the server issues it with an expiry, e.g. 1 hour by default), the user can reconnect without logging in again. If the JWT expired, the AuthContext would refresh it via the plugin before reconnecting (the AuthContext’s `refreshToken()` uses the Common Ground SDK to get fresh data and calls `/api/auth/session` again). Once refreshed, a new socket connection can be made with the new token. The Socket.IO flow accounts for this by simply rejecting unauthorized connections – the client should handle getting a new token then retry connecting.

## Code Structure and Middleware

Organizing the code for clarity and maintainability is important. We can structure the real-time feature as follows:

* **Server Initialization**: A file (e.g. `socketServer.ts`) that sets up the `io` instance and configures the Redis adapter and middleware. This could be invoked in a custom server script or within an API route on first use. It will export the `io` object for use elsewhere (for example, API routes might import `io` to emit events on data changes).

* **Authentication Middleware**: As shown above, a small function to verify JWT and attach `socket.data.user`. This lives in the server init code (`io.use(authMiddleware)`).

* **Event Handlers**: We can group socket event logic by domain. For instance, a module `realtime/postEvents.ts` could handle events related to posts: join/leave board, newPost broadcast, postVoted, etc. Another `realtime/commentEvents.ts` for comments and perhaps typing. These modules would register listeners on `io` or on each socket within the `io.on('connection')` callback. For example, in `io.on('connection', socket => { ... })`, we can call functions that attach all necessary event handlers to that socket. This keeps the `connection` callback tidy:

  ```js
  io.on('connection', socket => {
    registerBoardHandlers(socket);   // joinBoard, leaveBoard
    registerPostHandlers(socket);    // maybe none needed for client -> server, since posts come via REST
    registerCommentHandlers(socket); // for typing indications perhaps
    registerPresenceHandlers(socket); // could be integrated in join/leave
    // etc.
  });
  ```

  Many real-time events (new post, new comment, vote) are actually triggered server-side by API calls, not by direct socket messages from clients, so those “handlers” are simply the API calling `io.emit`. We can expose utility functions in these modules for the REST API to call. For example, a `postEvents.ts` might export `emitNewPost(boardId, postData)` which internally does `io.to(boardRoom).emit('newPost', postData)`. The API route for creating a post can import and call that after a successful DB insert. This decouples the API logic from socket logic, improving testability.

* **Using Existing Libraries**: There are some libraries to aid with Socket.IO and JWT auth (e.g. `socketio-jwt-auth`) and rate-limiting (like `@d3vision/socket.io-rate-limiter`). For MVP, we might implement it ourselves to keep things simple and transparent. The JWT verification via `jsonwebtoken` is already in use (same secret, etc.), so it’s easy to replicate. For rate limiting, instead of a library, a basic middleware using something like `rate-limiter-flexible` can throttle events per socket or IP.

* **Connecting Auth to Rooms**: We will reuse `canUserAccessBoard` and related logic from the existing codebase for permission enforcement. It might mean refactoring those helper functions (`boardPermissions.ts`) to be usable from the socket server context (which is essentially the same Node environment as API). For example, `canUserAccessBoard(roles, boardSettings, isAdmin)` is already defined and used in the API, so we just import it for the socket check. This ensures consistency in how permissions are evaluated across REST and WebSocket.

By structuring handlers in separate modules and reusing library code, we maintain the **single source of truth** for permissions and avoid duplicating logic.

## Rate Limiting and Abuse Prevention

Even at MVP stage, it’s wise to consider basic abuse prevention for the real-time system:

* **Event Emission Limits**: Some events are entirely server-driven (new posts, etc.), which ordinary users can’t directly flood except by spamming the actions themselves (which are already somewhat limited by API rate limiting or user interface). But events like **typing** or room joins could be spammed by a malicious client. We should implement *server-side throttling*: for instance, maintain a timestamp of the last `typing` event sent by each socket for each context, and if a new one comes too soon (e.g. within 200ms), ignore it. This prevents a spammy client from flooding the room with “typing” signals. Similarly, we could limit how frequently a user can join/leave rooms to prevent toggling.

* **Rate Limiting Connections**: Using the `rate-limiter-flexible` package or similar, we could limit how many new socket connections per IP or per userId are allowed per minute. This helps mitigate DoS attempts where someone might try to overwhelm the server with connections. Socket.IO’s middleware can be used to check the connecting IP against a Redis-stored counter and reject if too frequent.

* **Message Rate Limit**: For user-originated messages (if any were to be added, like chat or frequent data updates), we’d enforce a limit per second. In our current scope, direct user emissions are minimal (mostly “joinBoard” and “typing”). We can comfortably code a simple check: e.g., allow at most 5 `typing` events per 10 seconds per user.

* **Validation**: Continue to validate inputs on socket events. Just as the API validates request bodies, the socket handlers should validate data types and lengths (for example, ensure a boardId is a number and belongs to a certain range, etc., to prevent injection or weird behavior).

* **Using Built-in Tools**: The Socket.IO docs recommend using middleware for rate limiting as well. We could integrate an off-the-shelf middleware like `socket.io-rate-limiter` or simply use an in-memory token bucket. Since we plan to use Redis, an advanced approach is to use Redis also for rate-limit counters (so that if we scale to multiple instances, the limits are global). Given MVP scope, even an in-memory per-instance limit is okay (the scale might be small initially).

* **CORS and Origins**: Ensure the Socket.IO server only accepts connections from allowed origins (e.g., the domain of the web app or the parent platform). This prevents random external sites from connecting to the socket.

In summary, we add a layer of basic filtering to avoid obvious abuse patterns, without over-engineering. As the application grows, these limits can be tightened or expanded into a more sophisticated system (like user-level quotas, banning, etc.).

---

By implementing the above components, we will have a robust **MVP-ready real-time system** for Curia that aligns with the current architecture and security model. The system uses **self-hosted** infrastructure (no external push services) – just Socket.IO for websockets and Redis for scaling and pub/sub – all of which can be deployed on Railway (e.g., a Redis addon and the Node app in a Docker container). The solution honors **permissions** (only delivering events to authorized users), provides **live updates** for collaborative interaction, and integrates seamlessly with the existing **TanStack Query** caching and optimistic UI patterns to ensure a smooth user experience.
